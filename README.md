# Vision Transformer Model Training Examples

![Unknown](https://github.com/user-attachments/assets/a166bb6d-881a-4db2-88e1-198f9ba5339d)

<p><i>Vision Transformers</i> (ViTs) use the transformer architecture, originally designed for natural language processing, to process image data. An image is divided into fixed-size patches, which are flattened and linearly embedded into vectors. These vectors, along with positional encodings, are passed through multiple transformer layers. Each layer uses self-attention mechanisms to capture relationships between patches and feed-forward networks to process the features. The output of the final layer is passed to a classification head for prediction. ViTs excel at capturing global context in images due to their ability to focus on relevant regions across the entire input.</p>

These are some examples of using a Vision-Transformer Model for binary image classification.
<ul>
  <li>MNSIST : <a href='https://www.kaggle.com/datasets/oddrationale/mnist-in-csv'>MNIST Dataset on Kaggle</a></li>
  <li>RSNA Pneumonia : <a href='https://www.kaggle.com/competitions/rsna-pneumonia-detection-challenge'>RSNA Dataset on Kaggle</a></li>
</ul>
